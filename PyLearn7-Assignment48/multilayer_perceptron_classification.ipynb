{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":1142,"status":"ok","timestamp":1719725789979,"user":{"displayName":"Mahdi Ahmadi Kamal","userId":"02908466264960576341"},"user_tz":-210},"id":"S-kCkE701v61"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.datasets import load_digits\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":5,"status":"ok","timestamp":1719725789979,"user":{"displayName":"Mahdi Ahmadi Kamal","userId":"02908466264960576341"},"user_tz":-210},"id":"7Hko7VehAFpt","outputId":"dc061d65-465a-4e33-a94e-b24d5eb6e74a"},"outputs":[{"data":{"text/plain":["((1437, 64), (360, 64), (1437, 10), (360, 10))"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["dataset = load_digits()\n","X = dataset.data\n","Y = dataset.target\n","Y = np.eye(10)[Y]       # one hot\n","\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n","X_train.shape, X_test.shape, Y_train.shape, Y_test.shape"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1719725789980,"user":{"displayName":"Mahdi Ahmadi Kamal","userId":"02908466264960576341"},"user_tz":-210},"id":"Ecvt9iaEXVBY"},"outputs":[],"source":["def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))\n","\n","def softmax(x):\n","    return np.exp(x) / np.sum(np.exp(x))\n","\n","def root_mean_square_error(Y_gt, Y_pred):\n","    return np.sqrt(np.mean((Y_gt - Y_pred) ** 2))"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1719725789980,"user":{"displayName":"Mahdi Ahmadi Kamal","userId":"02908466264960576341"},"user_tz":-210},"id":"gGSvv7xNeE6i"},"outputs":[],"source":["epochs = 80\n","Î· = 0.001        # learning rate\n","\n","D_in = X_train.shape[1]               # Input layer number of neurons = 60\n","H1 = 128                              # Hidden layer 1 number of neurons\n","H2 = 32                               # Hidden layer 2 number of neurons\n","D_out = Y_train.shape[1]              # Output layer number of neurons = 10"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1719725789980,"user":{"displayName":"Mahdi Ahmadi Kamal","userId":"02908466264960576341"},"user_tz":-210},"id":"7RlTlNbS2h2z"},"outputs":[],"source":["W1 = np.random.randn(D_in, H1)\n","W2 = np.random.randn(H1, H2)\n","W3 = np.random.randn(H2, D_out)"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1719725789980,"user":{"displayName":"Mahdi Ahmadi Kamal","userId":"02908466264960576341"},"user_tz":-210},"id":"zp_IkRt_FQer"},"outputs":[],"source":["B1 = np.random.randn(1, H1)\n","B2 = np.random.randn(1, H2)\n","B3 = np.random.randn(1, D_out)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21962,"status":"ok","timestamp":1719725811938,"user":{"displayName":"Mahdi Ahmadi Kamal","userId":"02908466264960576341"},"user_tz":-210},"id":"BJt9_qKoIU16","outputId":"a26554da-9316-4597-82b4-76b082604bd9"},"outputs":[{"name":"stdout","output_type":"stream","text":["loss train: 0.31856302166096584\n","accuracy train: 0.12804453723034098\n","loss test: 0.2981712332812791\n","accuracy test: 0.2111111111111111\n","loss train: 0.2904638001706226\n","accuracy train: 0.29575504523312457\n","loss test: 0.28215937619606435\n","accuracy test: 0.36666666666666664\n","loss train: 0.2723732120976076\n","accuracy train: 0.43910925539318024\n","loss test: 0.2655073797452336\n","accuracy test: 0.49444444444444446\n","loss train: 0.2556204739905037\n","accuracy train: 0.5358385525400139\n","loss test: 0.2506266057018869\n","accuracy test: 0.55\n","loss train: 0.24055188083367593\n","accuracy train: 0.6144745998608212\n","loss test: 0.23984433273587583\n","accuracy test: 0.6111111111111112\n","loss train: 0.2277090118217595\n","accuracy train: 0.6638830897703549\n","loss test: 0.23062501698187932\n","accuracy test: 0.6333333333333333\n","loss train: 0.21720470654816607\n","accuracy train: 0.6965901183020181\n","loss test: 0.22192280201698722\n","accuracy test: 0.6611111111111111\n","loss train: 0.2080239932303106\n","accuracy train: 0.7258176757132916\n","loss test: 0.21448968183333877\n","accuracy test: 0.6805555555555556\n","loss train: 0.19976674003619527\n","accuracy train: 0.7592205984690327\n","loss test: 0.20770898260392973\n","accuracy test: 0.7083333333333334\n","loss train: 0.19224276694404505\n","accuracy train: 0.7835768963117606\n","loss test: 0.2017677436803256\n","accuracy test: 0.725\n","loss train: 0.18532122180553334\n","accuracy train: 0.8002783576896312\n","loss test: 0.1963593984967996\n","accuracy test: 0.7388888888888889\n","loss train: 0.17917807319057055\n","accuracy train: 0.8169798190675017\n","loss test: 0.19128603990207724\n","accuracy test: 0.7638888888888888\n","loss train: 0.1733349300157798\n","accuracy train: 0.8281141266527487\n","loss test: 0.18652825375493337\n","accuracy test: 0.7777777777777778\n","loss train: 0.16789683261810445\n","accuracy train: 0.8427279053583855\n","loss test: 0.18201228081985388\n","accuracy test: 0.8\n","loss train: 0.16300890890429282\n","accuracy train: 0.8580375782881002\n","loss test: 0.1780457316519086\n","accuracy test: 0.8111111111111111\n","loss train: 0.1586225978956191\n","accuracy train: 0.8636047320807237\n","loss test: 0.17434184910734687\n","accuracy test: 0.8194444444444444\n","loss train: 0.15448001470827447\n","accuracy train: 0.8698677800974252\n","loss test: 0.17079382320952105\n","accuracy test: 0.8305555555555556\n","loss train: 0.15052948960886253\n","accuracy train: 0.8775226165622826\n","loss test: 0.1675051641943126\n","accuracy test: 0.8361111111111111\n","loss train: 0.14685069522335645\n","accuracy train: 0.8810020876826722\n","loss test: 0.1645946443225637\n","accuracy test: 0.8416666666666667\n","loss train: 0.143389186669823\n","accuracy train: 0.8900487125956854\n","loss test: 0.1617159671015651\n","accuracy test: 0.8472222222222222\n","loss train: 0.1401593909345692\n","accuracy train: 0.8970076548364648\n","loss test: 0.15926725907787656\n","accuracy test: 0.8527777777777777\n","loss train: 0.137151101921675\n","accuracy train: 0.9018789144050104\n","loss test: 0.15698994969083088\n","accuracy test: 0.8555555555555555\n","loss train: 0.13437648240783495\n","accuracy train: 0.9046624913013221\n","loss test: 0.1548513302134534\n","accuracy test: 0.8583333333333333\n","loss train: 0.13151967334339382\n","accuracy train: 0.9081419624217119\n","loss test: 0.15275514296892642\n","accuracy test: 0.8666666666666667\n","loss train: 0.1287797130768787\n","accuracy train: 0.9123173277661796\n","loss test: 0.1505532935589559\n","accuracy test: 0.8722222222222222\n","loss train: 0.126135353611331\n","accuracy train: 0.9164926931106472\n","loss test: 0.14829925279178816\n","accuracy test: 0.8694444444444445\n","loss train: 0.12355957786562108\n","accuracy train: 0.9220598469032707\n","loss test: 0.14617567985200536\n","accuracy test: 0.8694444444444445\n","loss train: 0.12112103867128673\n","accuracy train: 0.9227557411273486\n","loss test: 0.14432744137794487\n","accuracy test: 0.8777777777777778\n","loss train: 0.1188217011251953\n","accuracy train: 0.9262352122477383\n","loss test: 0.14264640128370584\n","accuracy test: 0.8833333333333333\n","loss train: 0.11665855248004198\n","accuracy train: 0.929714683368128\n","loss test: 0.1410615612545419\n","accuracy test: 0.8861111111111111\n","loss train: 0.11462442193421621\n","accuracy train: 0.930410577592206\n","loss test: 0.13959657740750253\n","accuracy test: 0.8861111111111111\n","loss train: 0.11270219836341966\n","accuracy train: 0.9338900487125957\n","loss test: 0.1382698472032726\n","accuracy test: 0.8833333333333333\n","loss train: 0.11086032395981998\n","accuracy train: 0.9366736256089074\n","loss test: 0.13701680626183205\n","accuracy test: 0.8888888888888888\n","loss train: 0.10908907927472564\n","accuracy train: 0.9373695198329853\n","loss test: 0.135823117885181\n","accuracy test: 0.8888888888888888\n","loss train: 0.10734203075557477\n","accuracy train: 0.9394572025052192\n","loss test: 0.13473805400704536\n","accuracy test: 0.8888888888888888\n","loss train: 0.10565185401470774\n","accuracy train: 0.941544885177453\n","loss test: 0.1337454799119744\n","accuracy test: 0.8888888888888888\n","loss train: 0.10401662863216973\n","accuracy train: 0.9443284620737648\n","loss test: 0.1328318036027224\n","accuracy test: 0.8916666666666667\n","loss train: 0.10228984531418465\n","accuracy train: 0.9457202505219207\n","loss test: 0.13190659681881922\n","accuracy test: 0.8888888888888888\n","loss train: 0.10065269498615985\n","accuracy train: 0.9491997216423104\n","loss test: 0.13097070011413334\n","accuracy test: 0.8916666666666667\n","loss train: 0.09906091692246814\n","accuracy train: 0.9519832985386222\n","loss test: 0.1300260625567767\n","accuracy test: 0.8972222222222223\n","loss train: 0.09755099418924391\n","accuracy train: 0.954070981210856\n","loss test: 0.12918736955587426\n","accuracy test: 0.8972222222222223\n","loss train: 0.09600000324147036\n","accuracy train: 0.9554627696590118\n","loss test: 0.12839528969268202\n","accuracy test: 0.9\n","loss train: 0.09446788432609556\n","accuracy train: 0.9568545581071677\n","loss test: 0.12754813706808057\n","accuracy test: 0.8972222222222223\n","loss train: 0.09301150121064994\n","accuracy train: 0.9582463465553236\n","loss test: 0.12668008394967126\n","accuracy test: 0.8972222222222223\n","loss train: 0.09162892599316447\n","accuracy train: 0.9596381350034795\n","loss test: 0.1258562175066188\n","accuracy test: 0.9\n","loss train: 0.09028425638939849\n","accuracy train: 0.9603340292275574\n","loss test: 0.12501785077572547\n","accuracy test: 0.9027777777777778\n","loss train: 0.08895610102500147\n","accuracy train: 0.9617258176757133\n","loss test: 0.12407860289108096\n","accuracy test: 0.9027777777777778\n","loss train: 0.08767199611705845\n","accuracy train: 0.9624217118997912\n","loss test: 0.12309930783785486\n","accuracy test: 0.9027777777777778\n","loss train: 0.08644627438092886\n","accuracy train: 0.9631176061238692\n","loss test: 0.12224281481127579\n","accuracy test: 0.9027777777777778\n","loss train: 0.08526292053486127\n","accuracy train: 0.964509394572025\n","loss test: 0.12148537066957406\n","accuracy test: 0.9083333333333333\n","loss train: 0.08410265891547247\n","accuracy train: 0.965205288796103\n","loss test: 0.12081479146449853\n","accuracy test: 0.9083333333333333\n","loss train: 0.08296202242889401\n","accuracy train: 0.9659011830201809\n","loss test: 0.12021032621085953\n","accuracy test: 0.9083333333333333\n","loss train: 0.0818894955191127\n","accuracy train: 0.9672929714683368\n","loss test: 0.11961833416472648\n","accuracy test: 0.9111111111111111\n","loss train: 0.08089364961983272\n","accuracy train: 0.9672929714683368\n","loss test: 0.11902054961984485\n","accuracy test: 0.9111111111111111\n","loss train: 0.07995084437748733\n","accuracy train: 0.9679888656924147\n","loss test: 0.11841530893922075\n","accuracy test: 0.9111111111111111\n","loss train: 0.07904240399831217\n","accuracy train: 0.9700765483646486\n","loss test: 0.1178035767051561\n","accuracy test: 0.9111111111111111\n","loss train: 0.07815627148025757\n","accuracy train: 0.9700765483646486\n","loss test: 0.11719505366234455\n","accuracy test: 0.9111111111111111\n","loss train: 0.0772863621673116\n","accuracy train: 0.9707724425887265\n","loss test: 0.1166060155775713\n","accuracy test: 0.9083333333333333\n","loss train: 0.07643182670337835\n","accuracy train: 0.9707724425887265\n","loss test: 0.11604551682491444\n","accuracy test: 0.9083333333333333\n","loss train: 0.07559296335827838\n","accuracy train: 0.9721642310368824\n","loss test: 0.11551947984477928\n","accuracy test: 0.9083333333333333\n","loss train: 0.07477155373678541\n","accuracy train: 0.9735560194850382\n","loss test: 0.1150361781065639\n","accuracy test: 0.9111111111111111\n","loss train: 0.07396567654252419\n","accuracy train: 0.9742519137091162\n","loss test: 0.11459341507301633\n","accuracy test: 0.9138888888888889\n","loss train: 0.07317842704284395\n","accuracy train: 0.9749478079331941\n","loss test: 0.11418245367330956\n","accuracy test: 0.9138888888888889\n","loss train: 0.07241086220305262\n","accuracy train: 0.9749478079331941\n","loss test: 0.11379787055553102\n","accuracy test: 0.9138888888888889\n","loss train: 0.07165748866628423\n","accuracy train: 0.9756437021572721\n","loss test: 0.11343652569819507\n","accuracy test: 0.9138888888888889\n","loss train: 0.07091677238633895\n","accuracy train: 0.9756437021572721\n","loss test: 0.11309537322954977\n","accuracy test: 0.9138888888888889\n","loss train: 0.07018557439199544\n","accuracy train: 0.97633959638135\n","loss test: 0.1127638998653697\n","accuracy test: 0.9138888888888889\n","loss train: 0.06945989530035819\n","accuracy train: 0.9784272790535838\n","loss test: 0.11243070223079697\n","accuracy test: 0.9138888888888889\n","loss train: 0.06873819943127306\n","accuracy train: 0.9798190675017397\n","loss test: 0.11209879980236542\n","accuracy test: 0.9194444444444444\n","loss train: 0.06801957982993252\n","accuracy train: 0.9798190675017397\n","loss test: 0.1117757303992499\n","accuracy test: 0.9194444444444444\n","loss train: 0.06730501499792978\n","accuracy train: 0.9798190675017397\n","loss test: 0.11146506731465584\n","accuracy test: 0.9222222222222223\n","loss train: 0.06660044847303456\n","accuracy train: 0.9812108559498957\n","loss test: 0.11116575663172072\n","accuracy test: 0.9222222222222223\n","loss train: 0.0659128070305675\n","accuracy train: 0.9812108559498957\n","loss test: 0.1108722976457286\n","accuracy test: 0.9222222222222223\n","loss train: 0.06523936472569761\n","accuracy train: 0.9826026443980515\n","loss test: 0.11057803934987676\n","accuracy test: 0.9222222222222223\n","loss train: 0.06456896029633849\n","accuracy train: 0.9832985386221295\n","loss test: 0.11027759262500934\n","accuracy test: 0.9222222222222223\n","loss train: 0.06389213242005344\n","accuracy train: 0.9839944328462074\n","loss test: 0.10996950363075235\n","accuracy test: 0.9194444444444444\n","loss train: 0.06321334930564325\n","accuracy train: 0.9846903270702854\n","loss test: 0.10966379894195306\n","accuracy test: 0.9222222222222223\n","loss train: 0.06254770622358863\n","accuracy train: 0.9846903270702854\n","loss test: 0.10937708192112691\n","accuracy test: 0.9222222222222223\n","loss train: 0.061899279758135695\n","accuracy train: 0.9846903270702854\n","loss test: 0.10912094660683695\n","accuracy test: 0.9222222222222223\n","loss train: 0.061270535577923536\n","accuracy train: 0.9853862212943633\n","loss test: 0.10889695079131548\n","accuracy test: 0.925\n"]}],"source":["for epoch in range(epochs):\n","\n","    # train\n","    Y_pred_train = []\n","    for x, y in zip(X_train, Y_train):\n","\n","        x = x.reshape(-1, 1)\n","\n","        # forward\n","\n","        # layer 1\n","        out1 = sigmoid(x.T @ W1 + B1)\n","\n","        # layer 2\n","        out2 = sigmoid(out1 @ W2 + B2)\n","\n","        # layer 3\n","        out3 = softmax(out2 @ W3 + B3)\n","        y_pred = out3\n","        Y_pred_train.append(y_pred)\n","\n","        # backpropagation\n","\n","        # layer 3\n","        error = -2 * (y - y_pred)               # root_mean_square_error derivative\n","        grad_B3 = error\n","        grad_W3 = out2.T @ error\n","\n","        # layer 2\n","        error = error @ W3.T * out2 * (1 - out2)\n","        grad_B2 = error\n","        grad_W2 = out1.T @ error\n","\n","        # layer 1\n","        error = error @ W2.T * out1 * (1 - out1)\n","        grad_B1 = error\n","        grad_W1 = x @ error\n","\n","        # update\n","\n","        # layer 1\n","        W1 -= Î· * grad_W1\n","        B1 -= Î· * grad_B1\n","\n","        # layer 2\n","        W2 -= Î· * grad_W2\n","        B2 -= Î· * grad_B2\n","\n","        # layer 3\n","        W3 -= Î· * grad_W3\n","        B3 -= Î· * grad_B3\n","\n","    # test\n","    Y_pred_test = []\n","    for x, y in zip(X_test, Y_test):\n","\n","        x = x.reshape(-1, 1)\n","\n","        # forward\n","\n","        # layer 1\n","        out1 = sigmoid(x.T @ W1 + B1)\n","\n","        # layer 2\n","        out2 = sigmoid(out1 @ W2 + B2)\n","\n","        # layer 3\n","        out3 = softmax(out2 @ W3 + B3)\n","        y_pred = out3\n","        Y_pred_test.append(y_pred)\n","\n","    Y_pred_train = np.array(Y_pred_train).reshape(-1, 10)\n","    loss_train = root_mean_square_error(Y_train, Y_pred_train)\n","    accuracy_train = np.sum(np.argmax(Y_train, axis=1)==np.argmax(Y_pred_train, axis=1)) / len(Y_train)\n","    print(\"loss train:\", loss_train)\n","    print(\"accuracy train:\", accuracy_train)\n","\n","    Y_pred_test = np.array(Y_pred_test).reshape(-1, 10)\n","    loss_test = root_mean_square_error(Y_test, Y_pred_test)\n","    accuracy_test = np.sum(np.argmax(Y_test, axis=1)==np.argmax(Y_pred_test, axis=1)) / len(Y_test)\n","    print(\"loss test:\", loss_test)\n","    print(\"accuracy test:\", accuracy_test)\n"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":369,"status":"ok","timestamp":1719725878842,"user":{"displayName":"Mahdi Ahmadi Kamal","userId":"02908466264960576341"},"user_tz":-210},"id":"8VYL3iDNG7xs","outputId":"6488df09-04cc-4917-b285-db8fb3ab5d5f"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[1.01385698e-05 9.86429934e-01 3.23371080e-03 1.01950344e-03\n","  8.26742524e-04 3.85986885e-05 1.15594628e-03 3.65172679e-05\n","  7.12562115e-03 1.23287425e-04]]\n","1\n"]}],"source":["import cv2\n","\n","image = cv2.imread(\"input/test4.png\")\n","image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","image = image.reshape(64, 1)\n","\n","x = image\n","# forward\n","\n","# layer 1\n","out1 = sigmoid(x.T @ W1 + B1)\n","\n","# layer 2\n","out2 = sigmoid(out1 @ W2 + B2)\n","\n","# layer 3\n","out3 = softmax(out2 @ W3 + B3)\n","y_pred = out3\n","print(y_pred)\n","print(np.argmax(y_pred))"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNCwa12uLmsTxQVZnrVAu1h","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":0}
